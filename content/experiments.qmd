---
title: Experiments
subtitle: "Exploring tools to extract species occurrences from a corpus of student papers"
page-layout: full
format:
  html:
    code-fold: true
---

## Getting to know LLMs and LangChain

A foundational element of using LangChain as a wrapper for large language models (LLMs) like GPT4 is the prompt. A prompt in the LLM context is just like a prompt in the usual context: it's a thoughtfully designed question meant to elicit a response. If you want to use an LLM to explore text, it is critically important to design an effective prompt that will help the model generate accurate and helpful responses. We are exploring the use of LLMs to help us "read" undergraduate student research papers in marine science and figure out if the paper contains a species occurrence. That is, did the student observe or collect a given species at a given place during the course of their research? If they did, that kind of information is a species occurrence (species + place + date).

To explore the potential for using LLMs in this work, we selected a few online tools that are designed to help the user ask questions about text provided to the application. We picked a few student papers at random (all open access) and iterated on a series of questions to learn how to engineer prompts that might give us the information we need to determine if a paper includes a species occurrence. This was our process and what we found.

## Tools we tried

### Chat My Data üìù  ChatPDF üìù  Ask My PDF

-   Chat My Data is here: <https://blog.langchain.dev/tutorial-chatgpt-over-your-data/>
-   ChatPDF is here: <https://www.chatpdf.com/>
-   Ask My PDF is here: <https://ask-my-pdf.streamlit.app/>

The first question we gave to each chat tool was, "*What is this paper about?*"

## Testing Architectures

Having experimented with prompts across some pre-made conversational tools, the next step is to explore different combinations of tools/methods for 1. Text splitting, 2. Embeddings, 3. Vector store, 4. Vector store query. We came up with four main options (below) with some possible variations (see the yellow arrows).
```{ojs}

neato`
digraph {
	labelloc = "b"
	fontname = Arial
	node [
		shape = rectangle
		width = 1.5
		color= lightgray
		style = filled
		fontname="Helvetica,Arial,sans-serif"
	]
	edge [
    len = 2 
    penwidth = 1.5 
    arrowhead=open
    color= darkgray
  ]
	start = regular
	normalize = 0

	subgraph cluster_0 {
		style=filled;
		color= deepskyblue;
		node [style=filled,color=white];
		SentenceTransformers -> SentenceTransformerEmbeddings -> Annoy -> MultiQueryRetriever;
		label = "Option #1";
	}

	subgraph cluster_1 {
		style=filled;
		color= yellowgreen;
		node [style=filled,color=white];
		TikToken -> OpenAIEmbeddings -> FAISS -> ContextualCompression;
		label = "Option #2";
	}

subgraph cluster_2 {
		style=filled;
		color= orange;
		node [style=filled,color=white];
		NLTK -> LlamaCCP -> Pinecone -> PineconeSelfQuerying;
		label = "Option #3";
	}

subgraph cluster_3 {
		style=filled;
		color= deeppink;
		node [style=filled,color=white];
		SpaCY -> SpaCYEmbeddings -> Chroma -> ChromaSelfQuerying;
		label = "Option #4";
	}


	source -> PyMuPDF;
  PyMuPDF -> SentenceTransformers;
  PyMuPDF -> TikToken;
  PyMuPDF -> NLTK;
  PyMuPDF -> SpaCY;
	FAISS -> MultiQueryRetriever [color = yellow]
  Annoy -> ContextualCompression [color = yellow]
  LlamaCCP -> FAISS [color = yellow]
  LlamaCCP -> Chroma [color = yellow] 
  SpaCYEmbeddings -> Pinecone [color = yellow] 
  OpenAIEmbeddings -> Annoy [color = yellow] 

	source [shape=Msquare];
}
`
```

```{ojs}
neato = require("@observablehq/graphviz@0.2")
```
