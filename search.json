[
  {
    "objectID": "content/experiments_1.html",
    "href": "content/experiments_1.html",
    "title": "Experiments",
    "section": "",
    "text": "A foundational element of using LangChain as a wrapper for large language models (LLMs) like GPT4 is the prompt. A prompt in the LLM context is just like a prompt in the usual context: it‚Äôs a thoughtfully designed question meant to elicit a response. If you want to use an LLM to explore text, it is critically important to design an effective prompt that will help the model generate accurate and helpful responses. We are exploring the use of LLMs to help us ‚Äúread‚Äù undergraduate student research papers in marine science and figure out if the paper contains a species occurrence. That is, did the student observe or collect a given species at a given place during the course of their research? If they did, that kind of information is a species occurrence (species + place + date).\nTo explore the potential for using LLMs in this work, we selected a few online tools that are designed to help the user ask questions about text provided to the application. We picked a few student papers at random (all open access) and iterated on a series of questions to learn how to engineer prompts that might give us the information we need to determine if a paper includes a species occurrence. This was our process and what we found."
  },
  {
    "objectID": "content/experiments_1.html#getting-to-know-llms-and-langchain",
    "href": "content/experiments_1.html#getting-to-know-llms-and-langchain",
    "title": "Experiments",
    "section": "",
    "text": "A foundational element of using LangChain as a wrapper for large language models (LLMs) like GPT4 is the prompt. A prompt in the LLM context is just like a prompt in the usual context: it‚Äôs a thoughtfully designed question meant to elicit a response. If you want to use an LLM to explore text, it is critically important to design an effective prompt that will help the model generate accurate and helpful responses. We are exploring the use of LLMs to help us ‚Äúread‚Äù undergraduate student research papers in marine science and figure out if the paper contains a species occurrence. That is, did the student observe or collect a given species at a given place during the course of their research? If they did, that kind of information is a species occurrence (species + place + date).\nTo explore the potential for using LLMs in this work, we selected a few online tools that are designed to help the user ask questions about text provided to the application. We picked a few student papers at random (all open access) and iterated on a series of questions to learn how to engineer prompts that might give us the information we need to determine if a paper includes a species occurrence. This was our process and what we found."
  },
  {
    "objectID": "content/experiments_1.html#tools-we-tried",
    "href": "content/experiments_1.html#tools-we-tried",
    "title": "Experiments",
    "section": "Tools we tried",
    "text": "Tools we tried\n\nChat My Data üìù ChatPDF üìù Ask My PDF\n\nChat My Data is here: https://blog.langchain.dev/tutorial-chatgpt-over-your-data/\nChatPDF is here: https://www.chatpdf.com/\nAsk My PDF is here: https://ask-my-pdf.streamlit.app/\n\nThe first question we gave to each chat tool was, ‚ÄúWhat is this paper about?‚Äù\n\n\n\n\n\n\n\n\n\n\nModel\nPaper\nWhat is this paper about?\nSummarize the paper concisely with reference to materials and methods.\n\n\n\n\n0\nPDF GPT\nfhl_2014_Charifson_34622.pdf; Snail Predation ...\nThis paper is about the potential for charact...\nThe paper discussed the morphometrics and con...\n\n\n1\nChat Your Data\nfhl_2014_Charifson_34622.pdf; Snail Predation ...\nThe paper is about an experimental study that ...\nThe paper acknowledges the support and facilit...\n\n\n2\nAsk My PDF\nfhl_2014_Charifson_34622.pdf; Snail Predation ...\nThe paper is about the relationship between pr...\nThe paper investigates the relationship betwee...\n\n\n\n\n\nComparing results for initial questions across the three tools.\n\n\nSource: experiments.ipynb"
  },
  {
    "objectID": "content/experiments_1.html#testing-architectures",
    "href": "content/experiments_1.html#testing-architectures",
    "title": "Experiments",
    "section": "Testing Architectures",
    "text": "Testing Architectures\nHaving experimented with prompts across some pre-made conversational tools, the next step is to explore different combinations of tools/methods for 1. Load, 2. Transform (Text splitting), 3. Embed, 4. Store, 5. Retrieve (Vector store query). We came up with four main options (below) with some possible variations (see the yellow arrows).\n\n\nCode\nneato`\ndigraph {\n    labelloc = \"b\"\n    fontname = Arial\n    node [\n        shape = rectangle\n        width = 1.5\n        color= lightgray\n        style = filled\n        fontname=\"Helvetica,Arial,sans-serif\"\n    ]\n    edge [\n    len = 2 \n    penwidth = 1.5 \n    arrowhead=open\n    color= darkgray\n  ]\n    start = regular\n    normalize = 0\n\n    subgraph cluster_0 {\n        style=filled;\n        color= deepskyblue;\n        node [style=filled,color=white];\n        SentenceTransformers -&gt; SentenceTransformerEmbeddings -&gt; Annoy -&gt; MultiQueryRetriever;\n        label = \"Option #1\";\n    }\n\n    subgraph cluster_1 {\n        style=filled;\n        color= yellowgreen;\n        node [style=filled,color=white];\n        TikToken -&gt; OpenAIEmbeddings -&gt; FAISS -&gt; ContextualCompression;\n        label = \"Option #2\";\n    }\n\nsubgraph cluster_2 {\n        style=filled;\n        color= orange;\n        node [style=filled,color=white];\n        NLTK -&gt; LlamaCCP -&gt; Chroma -&gt; ChromaSelfQuerying;\n        label = \"Option #3\";\n    }\n\nsubgraph cluster_3 {\n        style=filled;\n        color= deeppink;\n        node [style=filled,color=white];\n        SpaCY -&gt; SpaCYEmbeddings;\n        label = \"Option #4\";\n    }\n\n\n    source -&gt; PyMuPDF;\n  PyMuPDF -&gt; SentenceTransformers;\n  PyMuPDF -&gt; TikToken;\n  PyMuPDF -&gt; NLTK;\n  PyMuPDF -&gt; SpaCY;\n    FAISS -&gt; MultiQueryRetriever [color = yellow]\n  Annoy -&gt; ContextualCompression [color = yellow]\n  LlamaCCP -&gt; FAISS [color = yellow]\n  SpaCYEmbeddings -&gt; Chroma [color = yellow] \n  OpenAIEmbeddings -&gt; Annoy [color = yellow] \n\n    source [shape=Msquare];\n}\n`\n\n\n\n\n\n\n\n\n\nCode\nneato = require(\"@observablehq/graphviz@0.2\")"
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#edit-and-add-your-pages",
    "href": "content/add-content.html#edit-and-add-your-pages",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/plan.html",
    "href": "content/plan.html",
    "title": "Plan of Action (updated June 28, 2023)",
    "section": "",
    "text": "Read the student papers and collect ground truth data (10 each)\nConversational Experiment making use of transformer LLM process for ‚Äúassisted reading‚Äù of the papers\nIdentify entities & verify against authorities/knowledge bases\nInterface for validation of each species occurrence\nOCR process for other papers\n\n\n\nWe all develop a shared understanding of the papers and how they are structured (or not). Gather a list of questions that we can use to guide a conversational model.\n\n\n\nBuild an experiment with already OCR‚Äôd set of papers based on how we identified SPOCs in step 1.\n\n\n\nIndex all of the documents, ask for all the species occurrences across all the docs, in a list, with citation in the paper (paper ID, page). A button next to each one shows the excerpts from the paper with the evidence, highlighted. Link to review the full highlighted paper if necessary.‚Ä®\nComplex prompt to return dense chunks of information with species name, location, depth, habitat, etc. with explanation of the information source.‚Ä®\nConversational verification, one paper at a time. Prompt for the first best guess at SPOCs from the given paper, further interrogation (memory required) to verify.\n\n\n\n\n\nOur early experiments with OpenAI model to identify entities was very successful. We think this additional step will help us in the verification process by: 1. Making sure that the entities align with results of assisted reading 2. Allow us access to alternate name forms, visual verification (pictures!) and geo-location\n\n\n\nThis step brings together the assisted reading with the entity (species, place, habitat, etc.) identification to help with the verification of the species occurrence."
  },
  {
    "objectID": "content/plan.html#read-student-papers",
    "href": "content/plan.html#read-student-papers",
    "title": "Plan of Action (updated June 28, 2023)",
    "section": "",
    "text": "We all develop a shared understanding of the papers and how they are structured (or not). Gather a list of questions that we can use to guide a conversational model."
  },
  {
    "objectID": "content/plan.html#conversational-experiment",
    "href": "content/plan.html#conversational-experiment",
    "title": "Plan of Action (updated June 28, 2023)",
    "section": "",
    "text": "Build an experiment with already OCR‚Äôd set of papers based on how we identified SPOCs in step 1.\n\n\n\nIndex all of the documents, ask for all the species occurrences across all the docs, in a list, with citation in the paper (paper ID, page). A button next to each one shows the excerpts from the paper with the evidence, highlighted. Link to review the full highlighted paper if necessary.‚Ä®\nComplex prompt to return dense chunks of information with species name, location, depth, habitat, etc. with explanation of the information source.‚Ä®\nConversational verification, one paper at a time. Prompt for the first best guess at SPOCs from the given paper, further interrogation (memory required) to verify."
  },
  {
    "objectID": "content/plan.html#identify-entities",
    "href": "content/plan.html#identify-entities",
    "title": "Plan of Action (updated June 28, 2023)",
    "section": "",
    "text": "Our early experiments with OpenAI model to identify entities was very successful. We think this additional step will help us in the verification process by: 1. Making sure that the entities align with results of assisted reading 2. Allow us access to alternate name forms, visual verification (pictures!) and geo-location"
  },
  {
    "objectID": "content/plan.html#interface-for-validation",
    "href": "content/plan.html#interface-for-validation",
    "title": "Plan of Action (updated June 28, 2023)",
    "section": "",
    "text": "This step brings together the assisted reading with the entity (species, place, habitat, etc.) identification to help with the verification of the species occurrence."
  },
  {
    "objectID": "content/experiments.html",
    "href": "content/experiments.html",
    "title": "experiments",
    "section": "",
    "text": "pip install pandas"
  },
  {
    "objectID": "content/experiments.html#getting-to-know-llms-and-langchain",
    "href": "content/experiments.html#getting-to-know-llms-and-langchain",
    "title": "experiments",
    "section": "Getting to know LLMs and LangChain",
    "text": "Getting to know LLMs and LangChain\nA foundational element of using LangChain as a wrapper for large language models (LLMs) like GPT4 is the prompt. A prompt in the LLM context is just like a prompt in the usual context: it‚Äôs a thoughtfully designed question meant to elicit a response. If you want to use an LLM to explore text, it is critically important to design an effective prompt that will help the model generate accurate and helpful responses. We are exploring the use of LLMs to help us ‚Äúread‚Äù undergraduate student research papers in marine science and figure out if the paper contains a species occurrence. That is, did the student observe or collect a given species at a given place during the course of their research? If they did, that kind of information is a species occurrence (species + place + date).\nTo explore the potential for using LLMs in this work, we selected a few online tools that are designed to help the user ask questions about text provided to the application. We picked a few student papers at random (all open access) and iterated on a series of questions to learn how to engineer prompts that might give us the information we need to determine if a paper includes a species occurrence. This was our process and what we found."
  },
  {
    "objectID": "content/experiments.html#tools-we-tried",
    "href": "content/experiments.html#tools-we-tried",
    "title": "experiments",
    "section": "Tools we tried",
    "text": "Tools we tried\n\nChat My Data üìù ChatPDF üìù Ask My PDF\n\nChat My Data\nChatPDF\nAsk My PDF\n\nThe first question we gave to each chat tool was, ‚ÄúWhat is this paper about?‚Äù"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taxa Project",
    "section": "",
    "text": "Climate change is driving rapid changes in our biosphere on local and global scales. Our capacity to understand these shifts relies entirely upon two critical things, Long-term biological and environmental observations, and an ability to discover and access them. Species Occurrence records are foundational to understanding biodiversity in ecosystems and help researchers track adaptation and the effects of climate change.\nKnowledge bases that gather observations from around the world including species, location and time of the event provide a more complete picture of historical and geographic changes in biodiversity. Not surprisingly, observations from the past recorded on paper are often missing from these knowledge bases because they are hard to come by. Libraries up and down the Pacific coast hold collections of undergraduate student papers with observations of marine plants and animals ‚Äúhidden‚Äù in the text. Reading and extracting those observations by hand is an effort libraries cannot afford to undertake."
  },
  {
    "objectID": "index.html#motivations-for-project",
    "href": "index.html#motivations-for-project",
    "title": "Taxa Project",
    "section": "",
    "text": "Climate change is driving rapid changes in our biosphere on local and global scales. Our capacity to understand these shifts relies entirely upon two critical things, Long-term biological and environmental observations, and an ability to discover and access them. Species Occurrence records are foundational to understanding biodiversity in ecosystems and help researchers track adaptation and the effects of climate change.\nKnowledge bases that gather observations from around the world including species, location and time of the event provide a more complete picture of historical and geographic changes in biodiversity. Not surprisingly, observations from the past recorded on paper are often missing from these knowledge bases because they are hard to come by. Libraries up and down the Pacific coast hold collections of undergraduate student papers with observations of marine plants and animals ‚Äúhidden‚Äù in the text. Reading and extracting those observations by hand is an effort libraries cannot afford to undertake."
  },
  {
    "objectID": "index.html#goals-for-project",
    "href": "index.html#goals-for-project",
    "title": "Taxa Project",
    "section": "Goals for Project",
    "text": "Goals for Project\nThe goal of this project is to employ natural language processing, machine learning, and data visualization to amplify the work of librarians in identifying and verifying these observations.\nTo develop a dataset of species occurrences (a species at a given place at a specific time), we need to identify and extract scientifically relevant entities from the corpus. These entities include:\n\nTaxonomic names\nLocation(s) (named; ‚ÄúHopkins Beach‚Äù, ‚ÄúFisherman‚Äôs Cove‚Äù) and/or latitude/longitude\nA date when the species was observed\n\nOptionally, as supporting information, we can extract habitat type(s) (e.g., ‚Äúsubtidal‚Äù) from the text, too."
  },
  {
    "objectID": "index.html#student-papers-spanning-the-west-coast-of-north-america",
    "href": "index.html#student-papers-spanning-the-west-coast-of-north-america",
    "title": "Taxa Project",
    "section": "Student Papers spanning the West Coast of North America",
    "text": "Student Papers spanning the West Coast of North America\nThe Stanford Taxa Project is part of a larger effort (called Data Over Decades) with librarian partners up and down the western coast of North America. We each hold large collections of student research reports that contain observations of environmental conditions, species, and populations recorded over a span of at least nine decades. As we seek funding to support the digitization and preservation of collections from partner institutions, the Taxa Project explores the potential of computational methods to amplify the work of librarians in identifying and verifying species observations in student papers from Hopkins Marine Station of Stanford University (HMS). We hold 746 HMS student papers that were created from 1963 - 2011. Of these, 672 are open access, and 74 are restricted to use by Stanford-affiliated folks only."
  },
  {
    "objectID": "index.html#potential-corpus",
    "href": "index.html#potential-corpus",
    "title": "Taxa Project",
    "section": "Potential corpus",
    "text": "Potential corpus\nThe undergraduate student research papers available for the Data Over Decades project number into the thousands once more papers in these collections have been fully digitized (see Table 1). These papers were created annually, over several decades, as the capstone project for students taking an applied research course at their respective research station. For the Taxa Project, we are using the set of open access papers from Hopkins Marine Station as our initial corpus.\n\n\n\nInstitution\nApprox. papers\nDate Range\nDigital\nOCR‚Äôd\nOpen Access\n\n\n\n\nBamfield Marine Sciences Centre (consortium)\n5,426 papers\n1973 - present\nNo\nNo\nNo\n\n\nFriday Harbor Labs, University of Washington\n5,000 papers; 2011-2020, 480 papers in hand\n1946 - present\nPre-2011 not scanned; 2011+ born-digital\n2011-2020, 480 papers\nPre-2011, No; 2011+ Yes\n\n\nHatfield Marine Science Center, Oregon State University\n400 papers\n1967 - present\nNo\nNo\nNo\n\n\nBodega Marine Laboratory, UC Davis\n3,800 papers\n1928 - present\nNo\nNo\nNo\n\n\nMoss Landing Marine Labs, San Jose State University\n630 theses\n1999 - present\nNo\nNo\nNo\n\n\nUC Berkeley (research conducted at Hopkins)\n239 papers\n1947 - 1952\nNo\nNo\nNo\n\n\nHopkins Marine Station, Stanford University\n746 papers\n1963 - 2011**\nYes\nYes\n672 Yes; 74 No\n\n\nWrigley Marine Science Center, USC, UCLA, Carleton College (corpus held at HMS Library)\n300 papers\n1970s - 2000\nYes\nYes\nNo\n\n\nUC Santa Cruz (corpus held at HMS Library)\n59 papers\n1973 - 2000\nYes\nYes\nNo\n\n\n\nTable 1. A list of project partners in the Data Over Decades project, with an approximate number of papers at each site.\nGoogle Map of library partners in the Data Over Decades project."
  },
  {
    "objectID": "index.html#student-papers---from-paper-to-digital",
    "href": "index.html#student-papers---from-paper-to-digital",
    "title": "Taxa Project",
    "section": "Student Papers - From Paper to Digital",
    "text": "Student Papers - From Paper to Digital\nWe partnered with the Stanford Libraries Digital Production Group (DPG) to scan all of the Hopkins Marine Station student papers. The DPG is staffed by highly skilled professional imaging specialists and student assistants, and they have well-established workflows for capturing and processing digital content, and accessioning it into the Stanford Digital Repository (SDR). Their scanning process generated, for each page, a high-resolution JPEG 2000 for preservation, a medium resolution jpg for the catalog media viewer interface, an ALTO XML file, and a text-searchable PDF of the full paper. These files are all stored in Stanford Digital Repository (SDR) for preservation and access purposes.\nFor this phase of the project we will be working with the PDFs, but we plan to incorporate an open software image OCR step into our workflow so it will be more directly portable to our community of practice.\n\nüí° The following DPG staff contributed to the digitization and accessioning of the Hopkins Marine Station student papers: Katharine Dimitruk, Alexander Nguyen, Laura Nguyen, Linda Lam (supervisor)."
  },
  {
    "objectID": "architectures.html",
    "href": "architectures.html",
    "title": "Architecture options for taxa",
    "section": "",
    "text": "Optional Architectures\nThese are some of the options we want to try out and compare. The yellow lines indicate possible alternates.\n\nneato`\ndigraph {\n    labelloc = \"b\"\n    fontname = Arial\n    node [\n        shape = rectangle\n        width = 1.5\n        color= lightgray\n        style = filled\n        fontname=\"Helvetica,Arial,sans-serif\"\n    ]\n    edge [\n    len = 2 \n    penwidth = 1.5 \n    arrowhead=open\n    color= darkgray\n  ]\n    start = regular\n    normalize = 0\n\n    subgraph cluster_0 {\n        style=filled;\n        color= deepskyblue;\n        node [style=filled,color=white];\n        SentenceTransformers -&gt; SentenceTransformerEmbeddings -&gt; Annoy -&gt; MultiQueryRetriever;\n        label = \"Option #1\";\n    }\n\n    subgraph cluster_1 {\n        style=filled;\n        color= yellowgreen;\n        node [style=filled,color=white];\n        TikToken -&gt; OpenAIEmbeddings -&gt; FAISS -&gt; ContextualCompression;\n        label = \"Option #2\";\n    }\n\nsubgraph cluster_2 {\n        style=filled;\n        color= orange;\n        node [style=filled,color=white];\n        NLTK -&gt; LlamaCCP -&gt; Pinecone -&gt; PineconeSelfQuerying;\n        label = \"Option #3\";\n    }\n\nsubgraph cluster_3 {\n        style=filled;\n        color= deeppink;\n        node [style=filled,color=white];\n        SpaCY -&gt; SpaCYEmbeddings -&gt; Chroma -&gt; ChromaSelfQuerying;\n        label = \"Option #4\";\n    }\n\n\n    source -&gt; PyMuPDF;\n  PyMuPDF -&gt; SentenceTransformers;\n  PyMuPDF -&gt; TikToken;\n  PyMuPDF -&gt; NLTK;\n  PyMuPDF -&gt; SpaCY;\n    FAISS -&gt; MultiQueryRetriever [color = yellow]\n  Annoy -&gt; ContextualCompression [color = yellow]\n  LlamaCCP -&gt; FAISS [color = yellow]\n  LlamaCCP -&gt; Chroma [color = yellow] \n  SpaCYEmbeddings -&gt; Pinecone [color = yellow] \n  OpenAIEmbeddings -&gt; Annoy [color = yellow] \n\n    source [shape=Msquare];\n}\n`\n\n\n\n\n\n\n\nneato = require(\"@observablehq/graphviz@0.2\")"
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select ‚ÄúVersion Control‚Äù. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the ‚ÄúBuild‚Äù tab. Click the ‚ÄúRender website‚Äù button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify ‚Äúpreview in browser‚Äù or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select ‚ÄúShow output preview in: Viewer panel‚Äù."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don‚Äôt like using gh-pages?",
    "text": "Don‚Äôt like using gh-pages?\nIn some cases, you don‚Äôt want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won‚Äôt want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can‚Äôt use quarto publish gh-pages. You‚Äôll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don‚Äôt do this, but if you are not using the action (because it keeps failing), then you don‚Äôt need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#quarto-documentation",
    "href": "content/customizing.html#quarto-documentation",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people‚Äôs Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  }
]